# START OF FILE backend/blog_parser.py (ìƒëŒ€ ê²½ë¡œ ì„í¬íŠ¸ í•´ê²°)

import re
import time
import urllib.parse
import requests
from bs4 import BeautifulSoup
# ğŸ’¡ ìˆ˜ì •: constants.pyì—ì„œ DEFAULT_HEADERS ì„í¬íŠ¸ (ìƒëŒ€ ê²½ë¡œ ì œê±°)
from constants import DEFAULT_HEADERS 

# ê³µí†µ ë§ˆì»¤: ë‰´ìŠ¤/ë¸”ë¡œê·¸ì—ì„œ ë³¸ë¬¸ ë íŒë‹¨ìš©
NEWS_END_MARKERS = [
    "ë¬´ë‹¨ì „ì¬", "ë¬´ë‹¨ ì „ì¬", "ì¬ë°°í¬ ê¸ˆì§€", "â“’", "ì €ì‘ê¶Œì", "Copyright", "All rights reserved",
    "ê´‘ê³ ë¬¸ì˜", "ê´‘ê³  ë¬¸ì˜", "ADë§í¬", "íƒ€ë¶ˆë¼", "ê´€ë ¨ê¸°ì‚¬", "ê¸°ì E-mail", "ê¸°ì ì´ë©”ì¼",
    "ê¸°ìì†Œê°œ", "ê¸°ì ì†Œê°œ", "ê¸°ìì˜ ë‹¤ë¥¸ê¸°ì‚¬", "í¸ì§‘íŒ¨ë„", "ë³¸ë¬¸í•˜ë‹¨", "nBYLINE",
    "ì¢‹ì•„ìš” ë²„íŠ¼", "ì†ë³´ëŠ”", "t.me/", "í…”ë ˆê·¸ë¨", "ì˜ìƒì·¨ì¬", "ì˜ìƒí¸ì§‘", "ì‚¬ì§„ë¶€", "ì‚¬ì§„ê¸°ì", "ê·¸ë˜í”½=", "ì‚¬ì§„=",
    "ê¸°ì‚¬ì œë³´", "ë³´ë„ìë£Œ", "íŒŸìºìŠ¤íŠ¸", "ë§ì´ ë³¸ ê¸°ì‚¬", "ê³µìœ í•˜ê¸°", "ê³µìœ ë²„íŠ¼", "nCopyright",
    "ê¸°ì‚¬ ì „ì²´ë³´ê¸°", "ì…ë ¥ :", "ì§€ë©´ :", "AIí•™ìŠµ ì´ìš© ê¸ˆì§€", "AI í•™ìŠµìš© ë°ì´í„°", "AI ë°ì´í„° í™œìš©",
    "ê¸°ì‚¬ ê³µìœ ", "ëŒ“ê¸€", "ì¢‹ì•„ìš”", "ê´‘ê³ ", "ADVERTISEMENT", "ë°°ë„ˆ", "í›„ì›í•˜ê¸°", "ë”íŒ©íŠ¸ ì£¼ìš”ë‰´ìŠ¤",
    "ê´€ë ¨ ë‰´ìŠ¤", "ì¶”ì²œ ë‰´ìŠ¤", "ì‹¤ì‹œê°„ ì£¼ìš”ë‰´ìŠ¤", "ì£¼ìš”ë‰´ìŠ¤",
    "ë‰´ìŠ¤ì œê³µ", "ê¸°ì‚¬ì œê³µ", "ê¸°ì‚¬ í•˜ë‹¨ ê´‘ê³ ",
    "ê¸°ì‚¬ ì˜ì—­ í•˜ë‹¨ ê´‘ê³ ", "ê¸°ì ì •ë³´", "ì „ì²´ê¸°ì‚¬ ë³´ê¸°",
    "ì¥ê¸°ì˜ ê¸°ì", "â—", "ê³µê°ì–¸ë¡ ", "êµ¬ë…ì‹ ì²­", "ì§€ë©´PDF", "ë„¤ì´ë²„ í™ˆì—ì„œ", "ì¹´ì¹´ì˜¤í†¡ì—ì„œ", "ë§Œë‚˜ë³´ì„¸ìš”",
    "í•«ë‰´ìŠ¤", "â–¶", "â—", "â– ", "â–²", "â–¼", "â—†", "â—‡", "â˜", "â€»",
    "ì´ ê¸°ì‚¬ëŠ”", "ë³¸ ê¸°ì‚¬ëŠ”", "ìë£Œì¶œì²˜=", "ìë£Œ ì œê³µ="
]

def is_news_reporter_line(text: str) -> bool:
    """
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ê°€ ë‰´ìŠ¤ ê¸°ìì˜ ì„œëª…/ì •ë³´ ë¼ì¸ì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤.
    """
    return bool(re.search(r"[ê°€-í£]{2,5}\s*(ê¸°ì|íŠ¹íŒŒì›|ìœ„ì›|ë…¼ì„¤ìœ„ì›|ì—°êµ¬ì›|ê°ì›ê¸°ì|í†µì‹ ì›|ë°ì¼ë¦¬|ì¸í„´ê¸°ì|í¸ì§‘ì¥|ëŒ€í‘œ|êµìˆ˜|ë³€í˜¸ì‚¬|ì˜ì‚¬|ì•½ì‚¬|ë°•ì‚¬)", text))

def _clean_and_filter_text_from_elements(elements, url_for_debug: str, is_news: bool = False) -> str:
    """
    BeautifulSoupì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ ìš”ì†Œë“¤ì„ ê³µí†µ ê·œì¹™ì— ë”°ë¼ í´ë¦¬ë‹í•˜ê³  í•„í„°ë§í•©ë‹ˆë‹¤.
    ê³µí†µëœ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë¡œì§ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜.
    """
    cleaned_lines = []
    for el in elements:
        # íƒœê·¸ ë‚´ì˜ í…ìŠ¤íŠ¸ë¥¼ ê°œí–‰ ë¬¸ìë¡œ êµ¬ë¶„í•˜ì—¬ ê°€ì ¸ì˜¤ê³  ì–‘ìª½ ê³µë°± ì œê±°
        raw_text = el.get_text(separator="\n", strip=True)
        
        if not raw_text or len(raw_text) < 5: # ìµœì†Œ ê¸¸ì´ í•„í„°
            continue
        
        # ê¸°ì‚¬ ì¢…ë£Œ ë§ˆì»¤ ì²˜ë¦¬: ë§ˆì»¤ê°€ ë°œê²¬ë˜ë©´ ê·¸ ì´ì „ ë‚´ìš©ë§Œ ìœ íš¨í•œ ë³¸ë¬¸ìœ¼ë¡œ ê°„ì£¼
        for marker in NEWS_END_MARKERS:
            if marker.lower() in raw_text.lower():
                pos = raw_text.lower().find(marker.lower())
                # ë§ˆì»¤ê°€ ì‹œì‘ ë¶€ë¶„ì— ë„ˆë¬´ ê°€ê¹ì§€ ì•Šìœ¼ë©´ (ì¦‰, ë³¸ë¬¸ ì¼ë¶€ì¼ ê²½ìš°) ê·¸ ì´ì „ ë‚´ìš©ë§Œ ì‚¬ìš©
                raw_text = raw_text[:pos].strip() if pos > 10 else ""
                break
        
        if raw_text: # ë§ˆì»¤ ì²˜ë¦¬ í›„ ë‚´ìš©ì´ ë‚¨ì•„ìˆë‹¤ë©´
            # ë‰´ìŠ¤ ê¸°ìì˜ ì„œëª… ë¼ì¸ í•„í„°ë§ (ë‰´ìŠ¤ì¼ ê²½ìš°ì—ë§Œ ì ìš©)
            if is_news and is_news_reporter_line(raw_text) and len(raw_text) < 60:
                continue
            # ë¸”ë¡œê·¸ íŠ¹ìœ ì˜ ë¶ˆí•„ìš”í•œ ë¼ì¸ í•„í„°ë§ (ë‰´ìŠ¤ê°€ ì•„ë‹ ê²½ìš°, ì¦‰ ë¸”ë¡œê·¸ì¼ ê²½ìš° ì ìš©)
            if not is_news and (raw_text.startswith(("ëŒ“ê¸€", "ê³µê°", "íŠ¸ë™ë°±")) or re.match(r"^\d+ê°œì˜ ëŒ“ê¸€$", raw_text)):
                 continue
            
            cleaned_lines.append(raw_text)
            
    # í•„í„°ë§ëœ ë¼ì¸ë“¤ì„ ê°œí–‰ ë¬¸ìë¡œ ì¡°ì¸í•˜ê³  ìµœì¢…ì ìœ¼ë¡œ ì–‘ìª½ ê³µë°± ì œê±°
    return "\n".join(cleaned_lines).strip()


def get_content_from_html(soup: BeautifulSoup, post_url_for_debug: str = "") -> str | None:
    """
    ì£¼ì–´ì§„ BeautifulSoup ê°ì²´ì—ì„œ ë¸”ë¡œê·¸ ë³¸ë¬¸ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ìµœì‹  ë„¤ì´ë²„ ë¸”ë¡œê·¸ êµ¬ì¡°ì™€ êµ¬í˜• êµ¬ì¡° ëª¨ë‘ì— ëŒ€ì‘í•©ë‹ˆë‹¤.
    """
    # ìµœì‹  Naver ë¸”ë¡œê·¸ êµ¬ì¡° ìš°ì„  ì²˜ë¦¬ (se-viewer div.se-main-container)
    se_main_container = soup.select_one('div.se-viewer div.se-main-container')
    if se_main_container:
        # p.se-text-paragraph span ë‚´ë¶€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        content_parts = _clean_and_filter_text_from_elements(
            se_main_container.select('p.se-text-paragraph span'),
            post_url_for_debug, is_news=False # ë¸”ë¡œê·¸ì´ë¯€ë¡œ is_news=False
        )
        if content_parts: # ë‚´ìš©ì´ ì„±ê³µì ìœ¼ë¡œ ì¶”ì¶œë˜ë©´ ë°˜í™˜
            return content_parts

        # ë§Œì•½ p.se-text-paragraph spanì—ì„œ ì¶”ì¶œë˜ì§€ ì•Šì•˜ë‹¤ë©´, ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ìš”ì†Œ ì‹œë„
        other_text_elements_selectors = [
            'div.se-module-text', 'span[class*="se-fs-"]', 'span[class*="se-ff-"]',
            'div[class*="se-component-content"] div[class*="se-text"]', 'p[class*="se-text"]'
        ]
        other_content_parts = _clean_and_filter_text_from_elements(
            se_main_container.select(','.join(other_text_elements_selectors)),
            post_url_for_debug, is_news=False
        )
        if other_content_parts:
            return other_content_parts


    # êµ¬í˜• ë¸”ë¡œê·¸ êµ¬ì¡° ëŒ€ì‘ (ë ˆê±°ì‹œ ì…€ë ‰í„°ë“¤)
    legacy_content_selectors = [
        'div#postViewArea', 'div.post-view', 'div.se_component_wrap',
        'div.blog_content', 'article.contents_style', 'div.tt_article_useless_p_margin',
        'div.entry-content', 'div.td-post-content', 'div.article_view',
        'div.view', 'div.post_ct', 'td.bcc',
    ]
    for selector in legacy_content_selectors:
        container = soup.select_one(selector)
        if container:
            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±° (ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼, ê´‘ê³ , ë‚´ë¹„ê²Œì´ì…˜ ë“±)
            for s_tag in container.select('script, style, iframe, form, button, nav, header, footer, aside, .adsbygoogle, .revenue_unit_wrap, ins.kakao_ad_area, div[data-ke-type="revenue"]'):
                s_tag.decompose()
            
            # ì»¨í…Œì´ë„ˆ ë‚´ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ ë…¸ë“œë¥¼ ëŒ€ìƒìœ¼ë¡œ í´ë¦¬ë‹ ë° í•„í„°ë§
            meaningful_content = _clean_and_filter_text_from_elements(
                container.find_all(string=True, recursive=True), # ëª¨ë“  í…ìŠ¤íŠ¸ ë…¸ë“œë¥¼ ëŒ€ìƒìœ¼ë¡œ
                post_url_for_debug, is_news=False
            )
            if meaningful_content and len(meaningful_content) > 50: # ìµœì†Œ ê¸¸ì´ í™•ì¸
                return meaningful_content
            elif container.get_text(strip=True) and len(container.get_text(strip=True)) > 50:
                # ë§Œì•½ _clean_and_filter_text_from_elementsê°€ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•Šì•˜ì§€ë§Œ,
                # ì›ì‹œ í…ìŠ¤íŠ¸ê°€ ì¶©ë¶„íˆ ê¸´ ê²½ìš° fallbackìœ¼ë¡œ ì‚¬ìš©
                return container.get_text(strip=True)

    return None # ì–´ë–¤ ë‚´ìš©ë„ ì°¾ì§€ ëª»í•œ ê²½ìš° None ë°˜í™˜


def get_blog_post_content(post_url: str) -> str:
    """
    ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ URLì—ì„œ ë³¸ë¬¸ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ì˜ ê²½ìš° iframe ë‚´ë¶€ì˜ ì½˜í…ì¸ ë¥¼ ì¶”ê°€ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    headers = DEFAULT_HEADERS.copy()

    try:
        response = requests.get(post_url, headers=headers, timeout=15)
        response.raise_for_status() # HTTP ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì˜ˆì™¸ ë°œìƒ
        response.encoding = response.apparent_encoding # ì¸ì½”ë”© ìë™ ê°ì§€
        main_soup = BeautifulSoup(response.text, 'html.parser')

        iframe_tag = main_soup.select_one('iframe#mainFrame, iframe[name="mainFrame"]')
        content_soup = main_soup # ê¸°ë³¸ì ìœ¼ë¡œëŠ” ë©”ì¸ í˜ì´ì§€ì˜ soup ì‚¬ìš©
        actual_content_url = post_url

        # iframeì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°, iframe ë‚´ë¶€ì˜ srcë¥¼ íŒŒì‹±í•˜ì—¬ ì‹¤ì œ ì½˜í…ì¸ ë¥¼ ê°€ì ¸ì˜´
        if iframe_tag and iframe_tag.get('src'):
            iframe_src = iframe_tag['src']
            if iframe_src.startswith('/'): # ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
                parsed_url = urllib.parse.urlparse(post_url)
                iframe_src = f"{parsed_url.scheme}://{parsed_url.netloc}{iframe_src}"
            
            time.sleep(0.5) # iframe ìš”ì²­ ì „ ë”œë ˆì´ (ê³¼ë„í•œ ìš”ì²­ ë°©ì§€)
            iframe_headers = headers.copy()
            iframe_headers['Referer'] = post_url # Referer í—¤ë” ì¶”ê°€ (ì¼ë¶€ ì‚¬ì´íŠ¸ì—ì„œ í•„ìš”)
            iframe_response = requests.get(iframe_src, headers=iframe_headers, timeout=15)
            iframe_response.raise_for_status()
            iframe_response.encoding = iframe_response.apparent_encoding
            content_soup = BeautifulSoup(iframe_response.text, 'html.parser')
            actual_content_url = iframe_src

        content = get_content_from_html(content_soup, actual_content_url)
        if content is None or len(content.strip()) < 30: # ë³¸ë¬¸ ìµœì†Œ ê¸¸ì´ ê²€ì‚¬
            return f"ë¸”ë¡œê·¸ ë³¸ë¬¸ ë‚´ìš© ë¶€ì¡± ë˜ëŠ” ì»¨í…Œì´ë„ˆ ì—†ìŒ: {actual_content_url}"
        return content

    except requests.exceptions.Timeout:
        return f"ìš”ì²­ ì‹œê°„ ì´ˆê³¼ (ë¸”ë¡œê·¸): {post_url}"
    except requests.exceptions.SSLError as e:
        return f"SSL ì˜¤ë¥˜ (ë¸”ë¡œê·¸: {post_url}): {e}"
    except requests.exceptions.RequestException as e:
        return f"ìš”ì²­ ì˜¤ë¥˜ (ë¸”ë¡œê·¸: {post_url}): {e}"
    except Exception as e:
        return f"íŒŒì‹± ì˜¤ë¥˜ (ë¸”ë¡œê·¸: {post_url}): {e}"

# END OF FILE blog_parser.py
