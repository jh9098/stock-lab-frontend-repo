import time
import re
import urllib.parse
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import random
import os

from constants import DEFAULT_HEADERS
# from blog_parser import get_blog_post_content, _clean_and_filter_text_from_elements # GUIìš©ì´ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬
# from news_parser import extract_general_news_text, get_health_chosun_article_content # GUIìš©ì´ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬

# ğŸ’¡ ì¶”ê°€: requests.Session ê°ì²´ ìƒì„± (ëª¨ë“  ìš”ì²­ì— ì¬ì‚¬ìš©)
session = requests.Session()
session.headers.update(DEFAULT_HEADERS) # ê¸°ë³¸ í—¤ë”ë¥¼ ì„¸ì…˜ì— ì ìš©

# ë‰´ìŠ¤ ì•„ì´í…œ íŒŒì‹± ë¡œì§ì„ í—¬í¼ í•¨ìˆ˜ë¡œ ë¶„ë¦¬
def _parse_single_naver_news_item(item_soup, search_url):
    """
    í•˜ë‚˜ì˜ ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ ì•„ì´í…œì—ì„œ ì œëª©, ë§í¬, ì¶œì²˜, ë‚ ì§œ, ìš”ì•½ì„ íŒŒì‹±í•©ë‹ˆë‹¤.
    ìƒˆë¡œìš´ SDS UI êµ¬ì¡°ë¥¼ ìš°ì„  ì‹œë„í•˜ê³ , ê¸°ì¡´ ë ˆê±°ì‹œ UIë¥¼ í´ë°±ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    post_data = {
        'platform': 'news',
        'title': "ì œëª© ì—†ìŒ",
        'link': "ë§í¬ ì—†ìŒ",
        'source_name': "ì¶œì²˜ ì—†ìŒ",
        'post_date': "ë‚ ì§œ ì—†ìŒ",
        'content': "ìš”ì•½ ì—†ìŒ" # APIìš©ìœ¼ë¡œëŠ” ìš”ì•½ í…ìŠ¤íŠ¸ë§Œ í•„ìš”
    }

    # --- ìƒˆë¡œìš´ SDS (Search Design System) UI êµ¬ì¡° ìš°ì„  ì‹œë„ ---
    # ë‰´ìŠ¤ ì•„ì´í…œì˜ í•µì‹¬ ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ” ì»¨í…Œì´ë„ˆë¥¼ ë¨¼ì € ì°¾ìŠµë‹ˆë‹¤.
    # ì œê³µëœ HTMLì—ì„œ ì´ ì»¨í…Œì´ë„ˆëŠ” ë³´í†µ `sds-comps-vertical-layout sds-comps-full-layout` í´ë˜ìŠ¤ë¥¼ ê°€ì§‘ë‹ˆë‹¤.
    # ê·¸ë¦¬ê³  ê·¸ ì•ˆì— ì œëª©, ìš”ì•½, ì–¸ë¡ ì‚¬ ì •ë³´ê°€ í¬í•¨ë©ë‹ˆë‹¤.

    # ì œëª©: <span class="sds-comps-text-type-headline1">
    title_el_sds = item_soup.select_one('span.sds-comps-text-type-headline1')
    # ìš”ì•½: <span class="sds-comps-text-type-body1">
    summary_el_sds = item_soup.select_one('span.sds-comps-text-type-body1')

    # ì–¸ë¡ ì‚¬: <div class="sds-comps-profile-info-title"> ì•ˆì˜ <span> ë˜ëŠ” <a> ì•ˆì˜ <span>
    press_el_sds = item_soup.select_one('div.sds-comps-profile-info-title span.sds-comps-text-type-body2')

    # ë‚ ì§œ: <div class="sds-comps-profile-info"> ì•ˆì˜ <span class="sds-comps-profile-info-subtext"> ì•ˆì˜ <span>
    date_info_subtexts_sds = item_soup.select('div.sds-comps-profile-info span.sds-comps-profile-info-subtext')

    # ì›ë³¸ ê¸°ì‚¬ ë§í¬ (ì œëª©ì„ ê°ì‹¸ëŠ” <a> íƒœê·¸ ë˜ëŠ” 'ë„¤ì´ë²„ë‰´ìŠ¤' ë§í¬)
    # ì œëª© íƒœê·¸ë¥¼ ê°ì‹¸ëŠ” <a> íƒœê·¸ì—ì„œ ì§ì ‘ ë§í¬ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê²ƒì´ ê°€ì¥ í™•ì‹¤í•©ë‹ˆë‹¤.
    title_link_parent_sds = None
    if title_el_sds:
        title_link_parent_sds = title_el_sds.find_parent('a')

    # 'ë„¤ì´ë²„ë‰´ìŠ¤' ë§í¬ (link[href*="n.news.naver.com"]ê°€ ë” ì •í™•)
    naver_news_link_el_sds = item_soup.select_one('a[href*="n.news.naver.com"]')

    if title_el_sds:
        post_data['title'] = title_el_sds.get_text(strip=True)

        # ë§í¬ ìš°ì„ ìˆœìœ„: ë„¤ì´ë²„ë‰´ìŠ¤ ë§í¬ > ì œëª© íƒœê·¸ì˜ ë¶€ëª¨ ë§í¬
        if naver_news_link_el_sds and naver_news_link_el_sds.get('href'):
            post_data['link'] = naver_news_link_el_sds['href']
        elif title_link_parent_sds and title_link_parent_sds.get('href'):
            post_data['link'] = title_link_parent_sds['href']

        if press_el_sds: post_data['source_name'] = press_el_sds.get_text(strip=True)

        date_text_sds = None
        if date_info_subtexts_sds:
            for subtext_span_sds in date_info_subtexts_sds:
                actual_text_span_sds = subtext_span_sds.select_one('span.sds-comps-text-type-body2')
                if actual_text_span_sds:
                    date_text_candidate_sds = actual_text_span_sds.get_text(strip=True)
                    # 'ë„¤ì´ë²„ë‰´ìŠ¤'ë‚˜ ìˆ«ìë§Œ ìˆëŠ” 'në©´'ì€ ë‚ ì§œê°€ ì•„ë‹˜
                    if re.search(r'(\d{1,2}(ì‹œê°„|ë¶„) ì „|\d{4}\.\d{2}\.\d{2}\.?)$', date_text_candidate_sds) and "ë„¤ì´ë²„ë‰´ìŠ¤" not in date_text_candidate_sds and not re.search(r'\d+ë©´', date_text_candidate_sds):
                        date_text_sds = date_text_candidate_sds
                        break
        if date_text_sds: post_data['post_date'] = date_text_sds

        if summary_el_sds: post_data['content'] = summary_el_sds.get_text(strip=True) # ìš”ì•½ ì €ì¥ (APIìš©)
        # content í•„ë“œëŠ” fetch_content ê°’ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì„¤ì •ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” summaryë§Œ ì±„ì›Œë„£ìŒ

    else: # ê¸°ì¡´ ë˜ëŠ” ë‹¤ë¥¸ êµ¬ì¡°ì˜ ì•„ì´í…œ (ë ˆê±°ì‹œ UI í˜¸í™˜ì„± ìœ ì§€)
        # ì´ ë¶€ë¶„ì€ í˜„ì¬ ì œê³µëœ HTMLì—ëŠ” í•´ë‹¹í•˜ì§€ ì•Šì§€ë§Œ, ë§Œì•½ì„ ìœ„í•´ ìœ ì§€í•©ë‹ˆë‹¤.
        title_el_legacy = item_soup.select_one('a.news_tit, a.sa_text_title, div.news_area > div.tit, div.news_wrap > div.news_tit')
        if not title_el_legacy:
            title_el_legacy = item_soup.select_one('a[class*="news_tit"]')

        if title_el_legacy: post_data['title'] = title_el_legacy.get_text(strip=True)

        naver_news_link_el_legacy = item_soup.select_one('div.info_group a.info[href*="n.news.naver.com"], a.info._sp_each_url[href*="n.news.naver.com"], a.news_tit[href*="n.news.naver.com"]')
        if not naver_news_link_el_legacy:
            link_candidates_legacy = item_soup.select('a[href*="n.news.naver.com"]')
            if link_candidates_legacy:
                for lc_legacy in link_candidates_legacy:
                    if lc_legacy.get_text(strip=True) == "ë„¤ì´ë²„ë‰´ìŠ¤":
                        naver_news_link_el_legacy = lc_legacy
                        break
                if not naver_news_link_el_legacy:
                     if len(link_candidates_legacy) > 0: # ì—¬ëŸ¬ n.news.naver.com ë§í¬ ì¤‘ ì²«ë²ˆì§¸
                         naver_news_link_el_legacy = link_candidates_legacy[0]

        if naver_news_link_el_legacy and naver_news_link_el_legacy.get('href'):
            post_data['link'] = naver_news_link_el_legacy['href']
        elif title_el_legacy and title_el_legacy.get('href') and post_data['link'] == "ë§í¬ ì—†ìŒ":
            post_data['link'] = title_el_legacy['href']

        press_el_legacy = item_soup.select_one('a.info.press, span.press, a.sa_text_press, div.press_logo img[alt]')
        if press_el_legacy:
            if press_el_legacy.name == 'img' and press_el_legacy.has_attr('alt'):
                post_data['source_name'] = press_el_legacy['alt'].strip()
            else:
                post_data['source_name'] = press_el_legacy.get_text(strip=True)

        date_el_candidates_legacy = item_soup.select('span.info, span.num, span.time, div.info_group > span:not([class])')
        for date_cand_el_legacy in date_el_candidates_legacy:
            date_text_legacy = date_cand_el_legacy.get_text(strip=True)
            if re.search(r'(\d{1,2}(ì‹œê°„|ë¶„) ì „|\d{4}\.\d{2}\.\d{2}\.?)$', date_text_legacy) and "ë„¤ì´ë²„ë‰´ìŠ¤" not in date_text_legacy and not re.search(r'\d+ë©´', date_text_legacy):
                post_data['post_date'] = date_text_legacy
                break

        summary_el_legacy = item_soup.select_one('div.news_dsc div.dsc_wrap a, a.api_txt_lines.dsc_txt, div.dsc_txt, a.sa_text_lede, div.news_wrap div.news_dsc a')
        if summary_el_legacy: post_data['content'] = summary_el_legacy.get_text(strip=True) # ìš”ì•½ ì €ì¥ (APIìš©)

    # ê³µí†µ ë§í¬ í›„ì²˜ë¦¬: ìƒëŒ€ ê²½ë¡œ ë§í¬ë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
    if post_data['link'] != "ë§í¬ ì—†ìŒ" and not post_data['link'].startswith('http'):
        if post_data['link'].startswith("/"): # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš°
            post_data['link'] = urllib.parse.urljoin("https://search.naver.com/", post_data['link'])

    return post_data

# --- GUI ì•±ì„ ìœ„í•œ í¬ë¡¤ë§ í•¨ìˆ˜ (í˜„ì¬ APIì™€ëŠ” ë¬´ê´€í•˜ì§€ë§Œ ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€ë¥¼ ìœ„í•´ ê·¸ëŒ€ë¡œ ë‘ ) ---
# scrape_content_for_gui í•¨ìˆ˜ëŠ” ë³€ê²½í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
def scrape_content_for_gui(keyword, num_posts, fetch_content, status_callback, item_processed_callback, search_type="blog"):
    """
    ê¸°ì¡´ Tkinter GUI ì•±ì„ ìœ„í•œ í¬ë¡¤ë§ í•¨ìˆ˜.
    ì´ í•¨ìˆ˜ëŠ” ì›¹ APIì— ì§ì ‘ì ìœ¼ë¡œ ì‚¬ìš©ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ, GUI ì•±ì˜ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ë™ì‘í•©ë‹ˆë‹¤.
    """
    sort_order_text = "ìµœì‹ ìˆœ"
    if search_type == "blog": sort_order_text = "ê´€ë ¨ë„ìˆœ"
    elif search_type == "news": sort_order_text = "ìµœì‹ ìˆœ"
    elif search_type == "health_chosun_food": sort_order_text = "ëª©ë¡ìˆœ"

    status_callback(f"'{keyword if search_type != 'health_chosun_food' else 'í—¬ìŠ¤ì¡°ì„  í‘¸ë“œ'}' {search_type.replace('_', ' ').title()} {sort_order_text} ê²€ìƒ‰ ì‹œì‘ (ìµœëŒ€ {num_posts}ê°œ)...")

    headers = DEFAULT_HEADERS.copy()

    search_soup = None
    search_url = ""

    if search_type == "health_chosun_food":
        search_url = "https://health.chosun.com/list.html?menu=01010102&more_menu=0101010220&nowcode=0101010220"
    elif search_type == "blog":
        encoded_keyword = urllib.parse.quote_plus(keyword)
        search_url = f"https://search.naver.com/search.naver?ssc=tab.blog.all&sm=tab_jum&query={encoded_keyword}&nso=so:r,p:all"
    elif search_type == "news":
        encoded_keyword = urllib.parse.quote_plus(keyword)
        # GUIì—ì„œëŠ” ìµœì‹ ìˆœìœ¼ë¡œ ê³ ì •
        search_url = f"https://search.naver.com/search.naver?where=news&query={encoded_keyword}&sm=tab_opt&sort=1"
    else:
        status_callback("ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²€ìƒ‰ íƒ€ì…ì…ë‹ˆë‹¤.")
        item_processed_callback(None, is_done=True, error_occurred=True); return

    try:
        # GUI ìš©ì—ì„œëŠ” verify=Falseë¥¼ ìœ ì§€ (ë¡œì»¬ í™˜ê²½ ë“±ì˜ SSL ë¬¸ì œ ê°€ëŠ¥ì„±)
        search_response = requests.get(search_url, headers=headers, verify=False, timeout=10)
        search_response.raise_for_status()

        if search_type == "health_chosun_food":
            if 'euc-kr' in search_response.headers.get('content-type', '').lower() or \
               (search_response.encoding and search_response.encoding.lower() == 'iso-8859-1'):
                try: search_html_content = search_response.content.decode('euc-kr')
                except UnicodeDecodeError: search_html_content = search_response.text
            else:
                search_response.encoding = search_response.apparent_encoding if search_response.apparent_encoding else 'utf-8'
                search_html_content = search_response.text
        else:
            search_response.encoding = 'utf-8'
            search_html_content = search_response.text
        search_soup = BeautifulSoup(search_html_content, 'html.parser')

    except requests.exceptions.RequestException as e:
        status_callback(f"'{search_type}' ëª©ë¡ í˜ì´ì§€ ìš”ì²­ ì˜¤ë¥˜: {e}")
        item_processed_callback(None, is_done=True, error_occurred=True); return
    except Exception as e_parse:
        status_callback(f"'{search_type}' ëª©ë¡ í˜ì´ì§€ íŒŒì‹± ì˜¤ë¥˜: {e_parse}")
        item_processed_callback(None, is_done=True, error_occurred=True); return

    items_found_soups = []
    if search_type == "blog":
        candidate_items = search_soup.select('div.view_wrap')
        if not candidate_items:
            candidate_items = search_soup.select('ul.lst_view > li.bx, div.lst_view > div.bx, ul.lst_total > li.bx, div.bx._svp_item')

        for item_candidate_soup in candidate_items:
            title_link_tag = item_candidate_soup.select_one('div.title_area a.title_link, div.title_area a.name_link')
            if not title_link_tag :
                 title_link_tag = item_candidate_soup.select_one('a.api_txt_lines.total_tit, a.link_tit')

            if title_link_tag and title_link_tag.get('href'):
                href_val = title_link_tag['href']
                if "blog.naver.com" in href_val:
                    items_found_soups.append(item_candidate_soup)
                    if len(items_found_soups) >= num_posts: break

    elif search_type == "news":
        # ğŸ’¡ GUIìš© ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ë„ ìƒˆë¡œìš´ êµ¬ì¡°ì— ë§ì¶° ìˆ˜ì •
        # ë©”ì¸ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆ
        fender_root_div = search_soup.select_one('div[data-fender-root="true"]')
        if fender_root_div:
            # ê°œë³„ ë‰´ìŠ¤ ì•„ì´í…œ ì»¨í…Œì´ë„ˆ (í´ë˜ìŠ¤ ëª… ë³€ê²½ì— ìœ ì˜)
            # `lc9J36iJ61J2vQuSLfFc`ëŠ” ë§¤ë²ˆ ë°”ë€” ìˆ˜ ìˆì§€ë§Œ, `desktop_mode api_subject_bx`ì™€ í•¨ê»˜ ìˆëŠ” ê°€ì¥ ë°”ê¹¥ì˜ ì»¨í…Œì´ë„ˆ ì•ˆì—ì„œ ì°¾ìŠµë‹ˆë‹¤.
            main_news_list_area = fender_root_div.select_one('div.lc9J36iJ61J2vQuSLfFc.desktop_mode.api_subject_bx')
            if main_news_list_area:
                # ê°œë³„ ë‰´ìŠ¤ ì•„ì´í…œì€ `sds-comps-vertical-layout sds-comps-full-layout` í´ë˜ìŠ¤ë¥¼ ê°€ì§„ divì…ë‹ˆë‹¤.
                items_found_soups = main_news_list_area.select('div.sds-comps-vertical-layout.sds-comps-full-layout')
                # ê·¸ë¦¬ê³  ì´ ì•„ì´í…œë“¤ ì¤‘ `data-sds-comp="Profile"`ì„ í¬í•¨í•˜ëŠ” ê²ƒë“¤ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤. (ë‰´ìŠ¤ ì•„ë‹Œ ê²ƒ ì œì™¸)
                items_found_soups = [item for item in items_found_soups if item.select_one('[data-sds-comp="Profile"]')]

        # ìƒˆ UIì—ì„œ ì•„ì´í…œì„ ëª» ì°¾ì•˜ì„ ê²½ìš°, ê¸°ì¡´ ë ˆê±°ì‹œ ì…€ë ‰í„°ë¡œ í´ë°±
        if not items_found_soups:
            legacy_news_containers = search_soup.select('ul.list_news li.bx, div.news_area')
            for container_item in legacy_news_containers:
                if container_item.select_one('a.news_tit, a.sa_text_title, div.news_area > div.tit'):
                    items_found_soups.append(container_item)

        if not items_found_soups:
            very_legacy_items = search_soup.select('div.news_list_api li, ul.list_news > li')
            if very_legacy_items:
                 items_found_soups.extend(very_legacy_items)

        items_found_soups = items_found_soups[:num_posts]

        if not items_found_soups:
             status_callback("ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡ ì»¨í…Œì´ë„ˆ ë˜ëŠ” ì•„ì´í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


    elif search_type == "health_chosun_food":
        article_list_ul = search_soup.select_one('ul#section-bottom-article-list')
        if article_list_ul: items_found_soups = article_list_ul.find_all('li', class_='rellist', limit=num_posts)
        else: status_callback("í—¬ìŠ¤ì¡°ì„  í‘¸ë“œ ëª©ë¡ ì»¨í…Œì´ë„ˆ(ul#section-bottom-article-list)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    if not items_found_soups:
        status_callback(f"'{search_type}' ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì•„ì´í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (0ê°œ)")
        if search_type == "blog" and keyword:
             safe_keyword = re.sub(r'[\\/*?:"<>|]', "", keyword).replace(' ','_')
             debug_filename = f"debug_blog_no_items_{safe_keyword}_{datetime.now().strftime('%H%M%S')}.html"
             try:
                 with open(debug_filename, "w", encoding="utf-8") as f:
                     f.write(search_soup.prettify())
                 print(f"DEBUG: ë¸”ë¡œê·¸ ì•„ì´í…œ 0ê°œ, HTML ì €ì¥ë¨: {debug_filename}")
             except Exception as e_file:
                 print(f"DEBUG: HTML íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e_file}")

        item_processed_callback(None, is_done=True, error_occurred=True); return

    total_items_to_process = len(items_found_soups)
    status_callback(f"ìˆ˜ì§‘ ëŒ€ìƒ '{search_type}' {total_items_to_process}ê°œ ë°œê²¬. ì²˜ë¦¬ ì‹œì‘...")
    processed_item_count = 0
    scraped_data = []

    for i, item_soup in enumerate(items_found_soups):
        if processed_item_count >= num_posts: break
        current_progress = f"'{search_type}' ì²˜ë¦¬ ì¤‘: {i+1}/{total_items_to_process}"
        post_data = {'platform': search_type, 'title': "ì œëª© ì—†ìŒ", 'link': "ë§í¬ ì—†ìŒ", 'source_name': "ì¶œì²˜ ì—†ìŒ", 'post_date': "ë‚ ì§œ ì—†ìŒ", 'content': "ìˆ˜ì§‘ ì•ˆ í•¨"}

        try:
            if search_type == "blog":
                title_el = item_soup.select_one('div.title_area a.title_link, div.title_area a.name_link, a.link_tit')
                if not title_el : title_el = item_soup.select_one('a.api_txt_lines.total_tit, a.dsc_link')

                if title_el:
                    post_data['title'] = title_el.get_text(strip=True)
                    post_data['link'] = title_el.get('href', "ë§í¬ ì—†ìŒ")

                user_info_area = item_soup.select_one('div.user_box_inner, div.user_info')
                if user_info_area:
                    name_el = user_info_area.select_one('a.name, a.user_name, a.name.elss, a.nickname')
                    if name_el: post_data['source_name'] = name_el.get_text(strip=True)

                    date_el = user_info_area.select_one('span.sub, span.date, span.time')
                    if date_el: post_data['post_date'] = date_el.get_text(strip=True)
                else:
                    user_info_view_tab = item_soup.select_one('div.info_group, div.etc_info')
                    if user_info_view_tab:
                        name_el_view = user_info_view_tab.select_one('a.name, a.writer')
                        if name_el_view: post_data['source_name'] = name_el_view.get_text(strip=True)
                        date_el_view = user_info_view_tab.select_one('span.date, span.time')
                        if date_el_view: post_data['post_date'] = date_el_view.get_text(strip=True)

            elif search_type == "news":
                # ê³µí†µ íŒŒì‹± í—¬í¼ í•¨ìˆ˜ ì‚¬ìš©
                parsed_news_item = _parse_single_naver_news_item(item_soup, search_url)
                post_data.update(parsed_news_item)
                # GUIì˜ contentëŠ” APIì˜ contentì™€ ë™ì¼í•˜ê²Œ ìš”ì•½ì„ ì €ì¥
                post_data['content'] = parsed_news_item['content']

            elif search_type == "health_chosun_food":
                title_tag = item_soup.select_one('div.inn > h4 > a')
                if title_tag:
                    post_data['title'] = title_tag.get_text(strip=True)
                    link_suffix = title_tag.get('href', "")
                    if link_suffix.startswith('/'): post_data['link'] = "https://health.chosun.com" + link_suffix
                    elif link_suffix.startswith('http'): post_data['link'] = link_suffix
                    else: post_data['link'] = urllib.parse.urljoin("https://health.chosun.com/", link_suffix)
                summary_tag = item_soup.select_one('div.inn > div.text > a')
                if summary_tag: post_data['content'] = summary_tag.get_text(strip=True) # GUIì—ì„œëŠ” contentì— ìš”ì•½
                author_tag = item_soup.select_one('div.em_area > span.name > a, div.em_area > span.name')
                if author_tag: post_data['source_name'] = author_tag.get_text(strip=True)
                else: post_data['source_name'] = "í—¬ìŠ¤ì¡°ì„ "
                date_tag = item_soup.select_one('div.em_area > span.date')
                if date_tag: post_data['post_date'] = date_tag.get_text(strip=True)

            # ë³¸ë¬¸ í¬ë¡¤ë§ (fetch_contentê°€ Trueì¼ ê²½ìš°ì—ë§Œ)
            if fetch_content and post_data.get('link') and post_data['link'] != "ë§í¬ ì—†ìŒ" and post_data['link'].startswith('http'):
                time.sleep(1.2) # ê³¼ë„í•œ ìš”ì²­ ë°©ì§€
                # í•„ìš”í•œ ê²½ìš° blog_parser, news_parser í•¨ìˆ˜ë¥¼ ì—¬ê¸°ì„œ import
                # ğŸ’¡ ìˆ˜ì •: í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ import
                from blog_parser import get_blog_post_content
                from news_parser import extract_general_news_text, get_health_chosun_article_content

                content_or_error = ""
                if search_type == "blog": content_or_error = get_blog_post_content(post_data['link'])
                elif search_type == "news":
                    content_or_error = extract_general_news_text(post_data['link'], summary_from_list=post_data.get('content')) # 'content'ë¡œ ë³€ê²½
                elif search_type == "health_chosun_food": content_or_error = get_health_chosun_article_content(post_data['link'])

                error_keywords_check = ["ìš”ì²­ ì‹œê°„ ì´ˆê³¼", "ìš”ì²­ ì˜¤ë¥˜", "íŒŒì‹± ì˜¤ë¥˜", "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨", "ì»¨í…Œì´ë„ˆ ì—†ìŒ", "ë‰´ìŠ¤ ë³¸ë¬¸ ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ë¸”ë¡œê·¸ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ì—†ìŒ", "ë¸”ë¡œê·¸ ë³¸ë¬¸ ë‚´ìš© ë¶€ì¡±", "í—¬ìŠ¤ì¡°ì„  ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨", "í—¬ìŠ¤ì¡°ì„  ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ì—†ìŒ", "SSL ì˜¤ë¥˜"]
                is_error_in_content = False
                if content_or_error:
                    for err_key in error_keywords_check:
                        if err_key in content_or_error:
                            is_error_in_content = True
                            break

                if content_or_error and not is_error_in_content:
                    post_data['content'] = content_or_error
                else:
                    post_data['content'] = content_or_error if content_or_error else "ë³¸ë¬¸ ì—†ìŒ (ì˜¤ë¥˜ ë˜ëŠ” ë‚´ìš© ì—†ìŒ)"
                    if content_or_error :
                        status_callback(f"{current_progress} - '{post_data['title'][:20]}...' ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {content_or_error[:50]}...")
                    else:
                        status_callback(f"{current_progress} - '{post_data['title'][:20]}...' ë³¸ë¬¸ ë‚´ìš© ì—†ìŒ.")

            elif not (post_data.get('link') and post_data['link'] != "ë§í¬ ì—†ìŒ" and post_data['link'].startswith('http')):
                 post_data['content'] = "ìœ íš¨í•œ ë§í¬ ì—†ì–´ ìˆ˜ì§‘ ë¶ˆê°€"

            scraped_data.append(post_data)
            processed_item_count +=1
            time.sleep(0.1) # ê° ì•„ì´í…œ ì²˜ë¦¬ í›„ ì§§ì€ ë”œë ˆì´
        except Exception as e_item:
            status_callback(f"{current_progress} - '{search_type}' ì•„ì´í…œ '{post_data.get('title', 'ì•Œ ìˆ˜ ì—†ìŒ')}' ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸: {e_item}")
            error_post_data = post_data.copy()
            error_post_data['content'] = f"ì•„ì´í…œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e_item}"
            item_processed_callback(error_post_data)
            continue

    status_callback("ëª¨ë“  ì•„ì´í…œ ì²˜ë¦¬ ì™„ë£Œ!")
    item_processed_callback(None, is_done=True)


# --- API í˜¸ì¶œì„ ìœ„í•œ ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜ (ëª©ë¡ìš©) ---
def fetch_naver_news_for_api(keyword, num_items):
    """ë„¤ì´ë²„ ê²€ìƒ‰ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì‹  ë‰´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    time.sleep(random.uniform(1.0, 3.0))

    client_id = os.getenv("NAVER_CLIENT_ID", "")
    client_secret = os.getenv("NAVER_CLIENT_SECRET", "")
    if not client_id or not client_secret:
        print("NAVER API ìê²© ì¦ëª…ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []

    api_url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
    }

    results = []
    start = 1
    display = 100

    while len(results) < num_items and start <= 1000:
        params = {
            "query": keyword,
            "display": min(display, num_items - len(results)),
            "start": start,
            "sort": "date",
        }

        try:
            resp = requests.get(api_url, headers=headers, params=params, timeout=10)
            resp.raise_for_status()
            items = resp.json().get("items", [])
        except requests.RequestException as e:
            print(f"ë„¤ì´ë²„ API ìš”ì²­ ì‹¤íŒ¨: {e}")
            break

        if not items:
            break

        for item in items:
            url = item.get("originallink") or item.get("link")
            if not url:
                continue

            try:
                pub = datetime.strptime(item.get("pubDate", ""), "%a, %d %b %Y %H:%M:%S %z")
                pub_date = pub.strftime("%Y-%m-%d")
            except Exception:
                pub_date = datetime.now().strftime("%Y-%m-%d")

            title = re.sub("<[^<]+?>", "", item.get("title", ""))
            summary = re.sub("<[^<]+?>", "", item.get("description", ""))
            source = urllib.parse.urlparse(url).netloc

            results.append({
                "platform": "news",
                "title": title,
                "link": url,
                "source_name": source,
                "post_date": pub_date,
                "content": summary,
            })

            if len(results) >= num_items:
                break

        start += display
        time.sleep(0.5)

    return results
