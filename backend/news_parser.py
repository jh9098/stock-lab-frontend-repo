# START OF FILE backend/news_parser.py (ìƒëŒ€ ê²½ë¡œ ì„í¬íŠ¸ í•´ê²°)

import re
import urllib.parse
import requests
from bs4 import BeautifulSoup
# ğŸ’¡ ìˆ˜ì •: blog_parser ì„í¬íŠ¸ë„ ì ˆëŒ€ ê²½ë¡œ ì„í¬íŠ¸ë¡œ
from blog_parser import is_news_reporter_line, NEWS_END_MARKERS, _clean_and_filter_text_from_elements
# ğŸ’¡ ìˆ˜ì •: constants.pyì—ì„œ DEFAULT_HEADERS ì„í¬íŠ¸ (ìƒëŒ€ ê²½ë¡œ ì œê±°)
from constants import DEFAULT_HEADERS 

def extract_general_news_text(url: str, summary_from_list: str = None) -> str:
    """
    ì¼ë°˜ ë‰´ìŠ¤ ê¸°ì‚¬ URLì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ë‹¤ì–‘í•œ ì–¸ë¡ ì‚¬ êµ¬ì¡°ì™€ ë„¤ì´ë²„ ë‰´ìŠ¤ íŠ¹ìœ ì˜ êµ¬ì¡°ì— ëŒ€ì‘í•©ë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ëŠ” GUI ì•±ì˜ ë³¸ë¬¸ ìˆ˜ì§‘ì— ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    try:
        response = requests.get(url, headers=DEFAULT_HEADERS, timeout=10)
        response.raise_for_status()

        html_content = response.text 

        soup = BeautifulSoup(html_content, "html.parser")
        domain = urllib.parse.urlparse(url).netloc
        article_text_element = None
        extracted_text = ""

        # Naver ë‰´ìŠ¤ ì „ìš© ì„ íƒì
        if "naver.com" in domain:
            naver_selectors = [
                "article#dic_area", "div#articleBodyContents", "div.go_trans._article_content",
                "div.newsct_article._article_body", "div#newsct_article", "div#articeBody"
            ]
            for sel in naver_selectors:
                article_text_element = soup.select_one(sel)
                if article_text_element:
                    break

            if article_text_element:
                # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                for tag in article_text_element.select("script, style, .media_end_head_autosummary, .promotion_area"):
                    tag.decompose()
                
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° í´ë¦¬ë‹ (í—¬í¼ í•¨ìˆ˜ ì‚¬ìš©)
                extracted_text = _clean_and_filter_text_from_elements(
                    article_text_element.find_all(string=True, recursive=True), url, is_news=True
                )

        else:
            # ì¼ë°˜ ì–¸ë¡ ì‚¬ë³„ CSS ì…€ë ‰í„° ë§¤í•‘
            selector_map = {
                "health.chosun.com": "div.par, div.article_body",
                "joongang.co.kr": "div.article_body",
                "hani.co.kr": "div.article-text",
                "kbs.co.kr": "div.view_con_text",
                "sbs.co.kr": "div.text_area",
                "ytn.co.kr": "div.content_area",
                "edaily.co.kr": "div.newsContents",
                "moneytoday.co.kr": "div.view_text",
                "kormedi.com": "div.news_view_content",
                "news1.kr": "div.detail",
                "segye.com": "div#article_txt",
                "yna.co.kr": "div.article",
            }

            matched = False
            for key, sel_str in selector_map.items():
                if key in domain:
                    selectors = [s.strip() for s in sel_str.split(',')]
                    for sel in selectors:
                        article_text_element = soup.select_one(sel)
                        if article_text_element:
                            matched = True
                            break
                if matched:
                    break

            if article_text_element:
                # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                for tag in article_text_element.select("script, style, form, iframe, .ad, .adsbygoogle"):
                    tag.decompose()
                
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° í´ë¦¬ë‹ (í—¬í¼ í•¨ìˆ˜ ì‚¬ìš©)
                extracted_text = _clean_and_filter_text_from_elements(
                    article_text_element.find_all(['p', 'div']), url, is_news=True
                )

        # ëŒ€ì•ˆ ë¡œì§: <p> íƒœê·¸ë§Œìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„
        if not extracted_text or len(extracted_text) < 50: # ë‚´ìš©ì´ ë¶€ì¡±í•œ ê²½ìš°ì—ë§Œ ëŒ€ì•ˆ ë¡œì§ ì‹¤í–‰
            p_tags = soup.find_all("p")
            content_from_p = _clean_and_filter_text_from_elements(p_tags, url, is_news=True)
            if content_from_p and len(content_from_p) > 100: # ëŒ€ì•ˆìœ¼ë¡œ ì¶”ì¶œí•œ ë³¸ë¬¸ì´ ì¶©ë¶„íˆ ê¸´ ê²½ìš°
                extracted_text = content_from_p

        return extracted_text if extracted_text and len(extracted_text) > 50 else f"ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ (ë‚´ìš© ë¶€ì¡± ë˜ëŠ” ëª¨ë“  ë°©ë²• ì‹¤íŒ¨): {url}"

    except requests.exceptions.Timeout:
        return f"ìš”ì²­ ì‹œê°„ ì´ˆê³¼ (ë‰´ìŠ¤): {url}"
    except requests.exceptions.SSLError as e:
        return f"SSL ì˜¤ë¥˜ (ë‰´ìŠ¤: {url}): {e}"
    except requests.exceptions.RequestException as e:
        return f"ìš”ì²­ ì˜¤ë¥˜ (ë‰´ìŠ¤: {url}): {e}"
    except Exception as e:
        return f"íŒŒì‹± ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ (ë‰´ìŠ¤: {url}): {e}"


def get_health_chosun_article_content(article_url: str) -> str:
    """
    'health.chosun.com' ì›¹ì‚¬ì´íŠ¸ì˜ ê¸°ì‚¬ ë³¸ë¬¸ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ëŠ” GUI ì•±ì˜ ë³¸ë¬¸ ìˆ˜ì§‘ì— ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    try:
        response = requests.get(article_url, headers=DEFAULT_HEADERS, timeout=10)
        response.raise_for_status()

        try:
            html = response.content.decode("euc-kr")
        except UnicodeDecodeError:
            response.encoding = response.apparent_encoding or 'utf-8'
            html = response.text

        soup = BeautifulSoup(html, "html.parser")
        container = soup.select_one("div.par, div.article_body")

        if not container:
            return f"í—¬ìŠ¤ì¡°ì„  ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ(div.par ë˜ëŠ” div.article_body) ì—†ìŒ: {article_url}"

        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for tag in container.select("script, style, form, iframe, .ad, .social_widget"):
            tag.decompose()

        extracted_content = _clean_and_filter_text_from_elements(
            container.find_all(['p', 'div']), article_url, is_news=True
        )

        return extracted_content if extracted_content else f"í—¬ìŠ¤ì¡°ì„  ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ (ë‚´ìš© ì—†ìŒ): {article_url}"

    except requests.exceptions.Timeout:
        return f"ìš”ì²­ ì‹œê°„ ì´ˆê³¼ (í—¬ìŠ¤ì¡°ì„ ): {article_url}"
    except requests.exceptions.SSLError as e:
        return f"SSL ì˜¤ë¥˜ (í—¬ìŠ¤ì¡°ì„ : {article_url}): {e}"
    except requests.exceptions.RequestException as e:
        return f"ìš”ì²­ ì˜¤ë¥˜ (í—¬ìŠ¤ì¡°ì„ : {article_url}): {e}"
    except Exception as e:
        return f"íŒŒì‹± ì˜¤ë¥˜ (í—¬ìŠ¤ì¡°ì„ : {article_url}): {e}"

# END OF FILE news_parser.py
